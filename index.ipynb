{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 Classification Trees: Perfect Split with Information Gain  - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we will simulate the example from the previous lesson in python. We will write functions to calculate entropy and IG which will be used for calculating these uncertainty measures and deciding upon creating a split using information gain while growing a ID3 classification tree. We shall attempt to write general function that can be used for other (larger)  problems as well. So let's get on with it. \n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "- Write functions for calculating Entropy and Information gain measures\n",
    "- Identify the attribute for best split at master and each subsequent node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "We shall use the same problem about deciding weather to go and play tennis on a given day, given the weather conditions. Here is the data from previous lesson:\n",
    "\n",
    "![](data.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function `entropy(pi)` to calculate total entropy in a given discrete probability distribution `pi`\n",
    "\n",
    "- The function should input a probability distribution `pi` as an array of class distributions\n",
    "- Calculate and return entropy according to the formula: $$Entropy(p) = -\\sum (P_i . log_2(P_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "def entropy(pi):\n",
    "    '''\n",
    "    return the Entropy of a probability distribution:\n",
    "    entropy(p) = - SUM (Pi * log(Pi) )\n",
    "    '''\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test the function \n",
    "\n",
    "print(entropy([1,1])) # Maximum Entropy e.g. a coin toss\n",
    "print (entropy([0,6])) # No entropy, ignore the -ve with zero , its there due to log function\n",
    "print (entropy([2,10])) # A random mix of classes\n",
    "\n",
    "# 1.0\n",
    "# 0.0\n",
    "# 0.6500224216483541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function `IG(D,a)` to calculate the information gain \n",
    "\n",
    "- The function should input `D` as a class distribution array for target class, and `a` the class distribution of the attribute to be tested\n",
    "- Using the `entropy()` function above, calculate the information gain as:\n",
    "\n",
    "$$gain(D,A) = Entropy(D) - \\sum(\\frac{|D_i|}{|D|}.Entropy(D_i))$$\n",
    "\n",
    "where `Di` represents distribution of each class in `a`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def IG(D, a):\n",
    "    '''\n",
    "    return the information gain:\n",
    "    gain(D, A) = entropy(D)− SUM( |Di| / |D| * entropy(Di) )\n",
    "    '''\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "# Uncomment to run the test\n",
    "\n",
    "# set of example of the dataset - distribution of classes\n",
    "test_dist = [6, 6] # Yes, No\n",
    "# attribute, number of members (feature)\n",
    "test_attr = [ [4,0], [2,4], [0,2] ] # class1, class2, class3 of attr1 according to YES/NO classes in test_dist\n",
    "\n",
    "print(IG(test_dist, test_attr))\n",
    "\n",
    "# 0.5408520829727552"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Iteration - Decide Best Split for master node\n",
    "\n",
    "- Create The class distribution `play` as a list showing frequencies of both classes from the dataset\n",
    "- Similarly create variables for four categorical feature attributes showing the class distribution for each class with respect to the target classes (yes and no)\n",
    "- Pass the play distribution with each attribute to calculate the information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain:\n",
      "\n",
      "Outlook: 0.2467498197744391\n",
      "Temperature: 0.029222565658954647\n",
      "Humidity: 0.15183550136234136\n",
      "Wind:, 0.04812703040826927\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Information Gain:\n",
    "\n",
    "# Outlook: 0.2467498197744391\n",
    "# Temperature: 0.029222565658954647\n",
    "# Humidity: 0.15183550136234136\n",
    "# Wind:, 0.04812703040826927"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the outlook attribute gives us the highest value for information gain, hence we choose this for creating a split at root node. So far we have our root node looking as below:\n",
    "![](tree-v1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Iteration\n",
    "\n",
    "For the branch (edge) of three above that leads to the \"Sunny\" outcome. Check for temperature, humidity and wind attributes to see which one provides the highest information gain. \n",
    "\n",
    "For the steps as above. __Remember we only have 2 positive and 3 negative examples in the \"sunny\" branch.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain:\n",
      "\n",
      "Temperature: 0.7974288158134881\n",
      "Humidity: 0.9402859586706309\n",
      "Wind:, 0.5117145300992023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Your code here \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Information Gain:\n",
    "\n",
    "# Temperature: 0.7974288158134881\n",
    "# Humidity: 0.9402859586706309\n",
    "# Wind:, 0.5117145300992023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we see that humidity gives us the the highest information gain, so we shall use this to split our tree as shown below:\n",
    "![](humid.png)\n",
    "\n",
    "Let's now see how to get to the leaf nodes using the branches from the node which we split on humidity above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Iteration\n",
    "\n",
    "We now have humidity which has two possible values [High, Normal]. A branch High dominated by single label which is __No__, caused this branch ended with a leaf contains label No. Same case with branch Normal which ended with a leaf contains label __Yes__. so we dont split these leaves any further as they are now \"pure leaves\" and will get shown as below:\n",
    "\n",
    "![](third.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth Iteration\n",
    "\n",
    "All rows contains value \"Overcast\" contain single label __Yes__ in the target, so branch of Overcast ends with a leaf contains label __Yes__. Just like above, it is a pure leaf and wont get split any further. \n",
    "\n",
    "![](fourth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Iteration\n",
    "\n",
    "We will now check which is the best attribute for branch of \"Rain\". Remember, that new distribution is only rows containing values of Rain i.e. 3 positive and 2 negative examples. Repeat the process we did for master node and second iteration to see which attribute gives the best information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain:\n",
      "\n",
      "Temperature: 0.01997309402197489\n",
      "Humidity: 0.01997309402197489\n",
      "Wind:, 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Information Gain:\n",
    "\n",
    "# Temperature: 0.01997309402197489\n",
    "# Humidity: 0.01997309402197489\n",
    "# Wind:, 0.9709505944546686"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, its wind this time, giving us the highest information gain. So that is what we use for the split. This will result as following:\n",
    "\n",
    "![](fifth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Iteration\n",
    "\n",
    "Next node is an attribute \"Wind\" with two possible classes [Weak, Strong]. A branch Strong dominated by single label which is __No__, caused this branch ended with a leaf contains label No. Same case with branch Weak which ended with a leaf contains label __Yes__. so no further splitting is required and we can declare these as leaf nodes as shown below.\n",
    "![](sixth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have all branches ending on leaf nodes, and these nodes are \"pure\" containing only positive or negative examples. So no further splitting is required and we now have a decision tree ready for classification. Also, notice that we did not use the the \"Temperature\" for splitting at any stage. We have effectively \"pruned\" that feature to leave it out of the training process as it we have better predictors helping us fully tidy up the data. We shall look more into pruning in following lessons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is a very simple model that you can build from starch easily. One of popular Decision Tree algorithm is ID3. Basically, we only need to construct tree data structure and implements two mathematical formula to build complete ID3 algorithm as we saw above. We shall look into this implementation with sklearn in our next lesson. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
